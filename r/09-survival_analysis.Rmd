# Survival Analysis for Modeling Singular Events Over Time {#survival}

`r if (knitr::is_latex_output()) '\\index{survival analysis|(}'`
In previous chapters, the outcomes we have been modeling have or have not occurred at a particular point in time following when the input variables were measured.  For example, in Chapter \@ref(linear-reg-ols) input variables were measured in the first three years of an education program and the outcome was measured at the end of the fourth year.  In many situations, the outcome we are interested in is a singular event that can occur at any time following when the input variables were measured, can occur at a different time for different individuals, and once it has occurred it cannot reoccur or repeat.  In medical studies, death can occur or the onset of a disease can be diagnosed at any time during the study period.  In employment contexts an attrition event can occur at various times throughout the year.  

An obvious and simple way to deal with this would be to simply agree to look at a specific point in time and measure whether or not the event had occurred at that point, for example 'How many employees had left at the three year point?'.  Such an approach allows us to use standard generic regression models like those studied in previous chapters.  But this approach has limitations.  

Firstly, we are only able to infer conclusions about the likelihood of the event having occurred as at the end of the period of study.  We cannot make inferences about the likelihood of the event throughout the period of study.  Being able to say that attrition is twice as likely for certain types of individuals *at any time throughout the three years* is more powerful than merely saying that attrition is twice as likely at the three year point.

Secondly, our sample size is constrained by the state of our data at the end of the period of study.  Therefore if we lose track of an individual after two years and six months, that observation needs to be dropped from our data set if we are focused only on the three year point.  Wherever possible, loss of data is something a statistician will want to avoid as it effects the accuracy and statistical power of inferences, and also means research effort was wasted. 

*Survival analysis* is a general term for the modeling of a time-associated binary non-repeated outcome, usually involving an understanding of the comparative risk of that outcome between two or more different groups of interest.  There are two common components in an elementary survival analysis, as follows:

* A graphical representation of the future outcome risk of the different groups over time, using *survival curves* based on `r if (knitr::is_latex_output()) '\\index{Kaplan-Meier estimates}'`Kaplan-Meier estimates of survival rate.  This is usually an effective way to establish *prima facie* relevance of a certain input variable to the survival outcome, and is a very effective visual way of communicating the relevance of the input variable to non-statisticians.
* A `r if (knitr::is_latex_output()) '\\index{Cox proportional hazard}'`*Cox proportional hazard* regression model to establish statistical significance of input variables and to estimate the effect of each input variable on the comparative risk of the outcome throughout the study period.  

Those seeking a more in depth treatment of survival analysis should consult texts on its use in medical/clinical contexts, and a recommended source is @collett. In this chapter we will use a walkthrough example to illustrate a typical use of survival analysis in a people analytics context.  

`r if (knitr::is_latex_output()) '\\index{data sets!job\\_retention@\\texttt{job\\_retention}}'`The `job_retention` data set shows the results of a study of around 3,800 individuals employed in various fields of employment over a 1 year period.  At the beginning of the study, the individuals were asked to rate their sentiment towards their job.  These individuals were then followed up monthly for a year to determine if they were still working in the same job or had left their job for a substantially different job.  If an individual was not successfully followed up in a given month, they were no longer followed up for the remainder of the study period.   

`r if (knitr::is_latex_output()) '\\small'`
```{r}
# if needed, get job_retention data
url <- "http://peopleanalytics-regression-book.org/data/job_retention.csv"
job_retention <- read.csv(url)
head(job_retention)
```
`r if (knitr::is_latex_output()) '\\normalsize'`

For this walkthrough example, the particular fields we are interested in are:

* `gender`:  The gender of the individual studied
* `field`: The field of employment that they worked in at the beginning of the study
* `level`:  The level of the position in their organization at the beginning of the study---`Low`, `Medium` or `High`.
* `sentiment`:   The sentiment score reported on a scale of 1 to 10 at the beginning of the study, with 1 indicating extremely negative sentiment and 10 indicating extremely positive sentiment
* `left`: A binary variable indicating whether or not the individual had left their job as at the last follow-up
* `month`:  The month of the last follow-up.  

## Tracking and illustrating survival rates over the study period

In our example, we are defining 'survival' as 'remaining in substantially the same job'.  We can regard the starting point as month 0 and we are following-up in each of months 1 through 12.  For a given month $i$, we can define a survival rate $S_i$ as follows:

$$
S_i = S_{i - 1}(1 - \frac{l_i}{n_i})
$$
where $l_i$ is the number reported as left in month $i$ and $n_i$ is the number still in substantially the same job after month $i - 1$, with $S_0 = 1$.

`r if (knitr::is_latex_output()) '\\index{R!packages!suvival@\\texttt{survival}}'`The `survival` package in R allows easy construction of survival rates on data in a similar format to that in our `job_retention` data set.  A survival object is created using the `Surv()` function to track the survival rate at each time period.

```{r, eval = FALSE}
library(survival)

# create survival object with event as 'left' and time as 'month'
retention <- Surv(event = job_retention$left, 
                  time = job_retention$month)

# view unique values of retention
unique(retention)
```

`r if (knitr::is_latex_output()) '\\small'`
```{r, echo = FALSE}
library(survival)

# create survival object with event as 'left' and time as 'month'
retention <- Surv(event = job_retention$left, 
                  time = job_retention$month)

# view unique values of retention
unique(retention)
```
`r if (knitr::is_latex_output()) '\\normalsize'`

We can see that our survival object records the month at which the individual had left their job if they are recorded as having done so in the data set.  If not, the object records the last month at which there was a record of the individual, appended with a '+' to indicate that this was the last record available.

`r if (knitr::is_latex_output()) '\\index{Kaplan-Meier estimates}'`The `survfit()` function allows us to calculate *Kaplan-Meier estimates* of survival for different groups in the data so that we can compare them.  We can do this using our usual formula notation but using a survival object as the outcome.  Let's take a look at survival by gender.

```{r}
# kaplan-meier estimates of survival by gender
kmestimate_gender <- survival::survfit(
  formula = Surv(event = left, time = month) ~ gender, 
  data = job_retention
)

summary(kmestimate_gender)

```

We can see that the `n.risk`, `n.event` and `survival` columns for each group correspond to the $n_i$, $l_i$ and $S_i$ in our formula above and that the confidence intervals for each survival rate are given. This can be very useful if we wish to illustrate a likely effect of a given input variable on survival likelihood.  

Let's imagine that we wish to determine if the sentiment of the individual had an impact on survival likelihood.  We can divide our population into two (or more) groups based on their sentiment and compare their survival rates.  

```{r}
# create a new field to define high sentiment (>= 7)
job_retention$sentiment_category <- ifelse(
  job_retention$sentiment >= 7, 
  "High", 
  "Not High"
)

# generate survival rates by sentiment category
kmestimate_sentimentcat <- survival::survfit(
  formula = Surv(event = left, time = month) ~ sentiment_category,
  data = job_retention
)

summary(kmestimate_sentimentcat)
```

`r if (knitr::is_latex_output()) '\\index{R!packages!survminer@\\texttt{survminer}}'`We can see that survival seems to consistently trend higher for those with high sentiment towards their jobs.  The `ggsurvplot()` function in the `survminer` package can visualize this neatly and also provide additional statistical information on the differences between the groups, as shown in Figure \@ref(fig:survivalplot).

```{r survivalplot, fig.align = "center", fig.cap = if (knitr::is_latex_output()) {"Survival curves by sentiment category in the \\texttt{job\\_retention} data"} else {"Survival curves by sentiment category in the `job_retention` data"}, out.width = if (knitr::is_latex_output()) {"90%"}}
library(survminer)

# show survival curves with p-value estimate and confidence intervals
survminer::ggsurvplot(
  kmestimate_sentimentcat,
  pval = TRUE,
  conf.int = TRUE,
  palette = c("blue", "red"),
  linetype = c("solid", "dashed"),
  xlab = "Month",
  ylab = "Retention Rate"
)

```
This confirms that the survival difference between the two sentiment groups is statistically significant and provides a highly intuitive visualization of the effect of sentiment on retention throughout the period of study.

## Cox proportional hazard regression models  {#coxphmodel}

`r if (knitr::is_latex_output()) '\\index{Cox proportional hazard}'`Let's imagine that we have a survival outcome that we are modeling for a population over a time $t$, and we are interested in how a set of input variables $x_1, x_2, \dots, x_p$ influences that survival outcome.  Given that our survival outcome is a binary variable, we can model survival at any time $t$ as a binary logistic regression.  We define $h(t)$ as the proportion who have not survived at time $t$, called the *hazard function*, and based on our work in Chapter \@ref(bin-log-reg):

$$
h(t) = h_0(t)e^{\beta_1x_1 + \beta_2x_2 + \dots + \beta_px_p}
$$
where $h_0(t)$ is a base or intercept hazard at time $t$, and $\beta_i$ is the coefficient associated with $x_i$ .

Now let's imagine we are comparing the hazard for two different individuals $A$ and $B$ from our population. We make an assumption that our hazard curves $h^A(t)$ for individual $A$ and $h^B(t)$ for individual $B$ are always proportional to each other and never cross---this is called the *proportional hazard assumption*. Under this assumption, we can conclude that:

$$
\begin{aligned}
\frac{h^B(t)}{h^A(t)} &= \frac{h_0(t)e^{\beta_1x_1^B + \beta_2x_2^B + \dots + \beta_px_p^B}}{h_0(t)e^{\beta_1x_1^A + \beta_2x_2^A + \dots + \beta_px_p^A}} \\
&= e^{\beta_1(x_1^B-x_1^A) + \beta_2(x_2^B-x_2^A) + \dots \beta_p(x_p^B-x_p^A)}
\end{aligned}
$$

Note that there is no $t$ in our final equation.  The important observation here is that the hazard for person B relative to person A is *constant and independent of time*.  This allows us to take a complicating factor out of our model.  It means we can model the effect of input variables on the hazard without needing to account for changes over times, making this model very similar in interpretation to a standard binomial regression model.

### Running a Cox proportional hazard regression model

A Cox proportional hazard model can be run using the `coxph()` function in the `survival` package, with the outcome as a survival object.  Let's model our survival against the input variables `gender`, `field`, `level` and `sentiment`.

```{r, eval = FALSE}
# run cox model against survival outcome
cox_model <- survival::coxph(
  formula = Surv(event = left, time = month) ~ gender + 
    field + level + sentiment,
  data = job_retention
)

summary(cox_model)
```

`r if (knitr::is_latex_output()) '\\small'`
```{r, echo = FALSE}
# run cox model against survival outcome
cox_model <- survival::coxph(
  formula = Surv(event = left, time = month) ~ gender + 
    field + level + sentiment,
  data = job_retention
)

summary(cox_model)
```
`r if (knitr::is_latex_output()) '\\normalsize'`

The model returns the following^[The concordance measure returned is a measure of how well the model can predict in any given pair who will survive longer, and is valuable in a number of medical research contexts]:

* Coefficients for each input variable and their p-values.  Here we can conclude that working in Finance or Health is associated with a significantly greater likelihood of leaving over the period studied, and that higher sentiment is associated with a significantly lower likelihood of leaving.

* Relative odds ratios associated with each input variable.  For example, a single extra point in sentiment reduces the odds of leaving by ~11%.  A single less point increases the odds of leaving by ~12%.  Confidence intervals for the coefficients are also provided.

* Three statistical tests on the null hypothesis that the coefficients are zero.  This null hypothesis is rejected by all three tests which can be interpreted as meaning that the model is significant. 

Importantly, as well as statistically validating that sentiment has a significant effect on retention, our Cox model has allowed us to control for possible mediating variables.  We can now say that sentiment has a significant effect on retention even for individuals of the same gender, in the same field and at the same level. 


### Checking the proportional hazard assumption

`r if (knitr::is_latex_output()) '\\index{Cox proportional hazard!proportional hazard assumption}'`Note that we mentioned in the previous section a critical assumption for our Cox proportional hazard model to be valid, called the proportional hazard assumption.  As always, it is important to check this assumption before finalizing any inferences or conclusions from your model.  

`r if (knitr::is_latex_output()) '\\index{Schoenfeld residual}'`The most popular test of this assumption uses a residual known as a *Schoenfeld residual* which would be expected to be independent of time if the proportional hazard assumption holds.  The `cox.zph()` function in the `survival` package runs a statistical test on the null hypothesis that the Schoenfeld residuals are independent of time.  The test is conducted on every input variable and on the model as a whole, and a significant result would reject the proportional hazard assumption.

```{r}
(ph_check <- survival::cox.zph(cox_model))
```

In our case, we can confirm that the proportional hazard assumption is not rejected.  The `ggcoxzph()` function in the `survminer` package takes the result of the `cox.zph()` check and allows a graphical check by plotting the residuals against time, as seen in Figure \@ref(fig:schoenfeld).

```{r schoenfeld, fig.align = "center", fig.cap = if (knitr::is_latex_output()) {"Schoenfeld test on proportional hazard assumption for \\texttt{cox\\_model}"} else {"Schoenfeld test on proportional hazard assumption for `cox_model`"}, out.width = if (knitr::is_latex_output()) {"90%"}}
survminer::ggcoxzph(ph_check, 
                    font.main = 10, 
                    font.x = 10, 
                    font.y = 10)
```

`r if (knitr::is_latex_output()) '\\newpage'`

## Frailty models

`r if (knitr::is_latex_output()) '\\index{frailty models}'`We noticed in our example in the previous section that certain fields of employment appeared to have a significant effect on the attrition hazard.  It is therefore possible that different fields of employment have different base hazard functions, and we may wish to take this into account in determining if input variables have a significant relationship with attrition.  This is analogous to a mixed model which we looked at in Section \@ref(mixed).

In this case we would apply a random intercept effect to the base hazard function $h_0(t)$ according to the field of employment of an individual, in order to take this into account in our modeling.  This kind of model is called a *frailty model*, taken from the clinical context, where different groups of patients may have different frailties (background risks of death).

There are many variants of how frailty models are run in the clinical context (see @collett for an excellent exposition of these), but the main application of a frailty model in people analytics would be to adapt a Cox proportional hazard model to take into account different background risks of the hazard event occurring among different groups in the data.  This is called a *shared frailty model*.   `r if (knitr::is_latex_output()) '\\index{R!packages!frailtypack@\\texttt{frailtypack}}'`The `frailtypack` R package allows various frailty models to be run with relative ease.  This is how we would run a shared frailty model on our `job_retention` data to take account of the different background attrition risk for the different fields of employment.


```{r, eval = FALSE, warning = FALSE, message = FALSE}
library(frailtypack)

(frailty_model <- frailtypack::frailtyPenal(
  formula = Surv(event = left, time = month) ~ gender + 
    level + sentiment + cluster(field),
  data = job_retention,
  n.knots = 12, 
  kappa = 10000
))
```

`r if (knitr::is_latex_output()) '\\scriptsize'`
```{r, echo = FALSE, warning = FALSE, message = FALSE}
library(frailtypack)

(frailty_model <- frailtypack::frailtyPenal(
  formula = Surv(event = left, time = month) ~ gender + 
    level + sentiment + cluster(field),
  data = job_retention,
  n.knots = 12, 
  kappa = 10000
))
```
`r if (knitr::is_latex_output()) '\\normalsize'`

We can see that the frailty parameter is significant, indicating that there is sufficient difference in the background attrition risk to justify the application of a random hazard effect.  We also see that the level of employment now becomes more significant in addition to sentiment, with Low and Medium level employees more likely to leave compared to High level employees.

The `frailtyPenal()` function can also be a useful way to observe the different baseline survivals for groups in the data.  For example, a simple stratified Cox proportional hazard model based on sentiment category can be constructed^[Note there needs to be a `kappa` for each level of the stratification].  

```{r, results = "hide"}
stratified_base <- frailtypack::frailtyPenal(
  formula = Surv(event = left, time = month) ~ 
    strata(sentiment_category),
  data = job_retention,
  n.knots = 12,
  kappa = rep(10000, 2)
)
```

This can then be plotted to observe how baseline retention differs by group, as in Figure \@ref(fig:stratified)^[This is another route to calculating survival curves similar to Figure \@ref(fig:survivalplot)].

```{r stratified, fig.pos = "!h", fig.align = "center", fig.cap = if (knitr::is_latex_output()) {"Baseline retention curves for the two sentiment categories in the \\texttt{job\\_retention} data set"} else {"Baseline retention curves for the two sentiment categories in the `job_retention` data set"}, out.width = if (knitr::is_latex_output()) {"90%"}}
plot(stratified_base, type.plot = "Survival", 
     pos.legend = "topright", Xlab = "Month",
     Ylab = "Baseline retention rate",
     color = 1)
```

`r if (knitr::is_latex_output()) '\\index{survival analysis|)}'`
`r if (knitr::is_latex_output()) '\\newpage'`

## Learning exercises

### Discussion questions

1. Describe some of the reasons why a survival analysis is a useful tool for analyzing data where outcome events happen at different times.
2. Describe the Kaplan-Meier survival estimate and how it is calculated.
3. What are some common uses for survival curves in practice?
4. Why is it important to run a Cox proportional hazard model in addition to calculating survival estimates when trying to understand the effect of a given variable on survival?
5. Describe the assumption that underlies a Cox proportional hazard model and how this assumption can be checked.
6. What is a frailty model and why might it be useful in the context of survival analysis?

### Data exercises

`r if (knitr::is_latex_output()) '\\index{data sets!job\\_retention@\\texttt{job\\_retention}}'`For these exercises, use the same `job_retention` data set as in the walkthrough example for this chapter, which can be loaded via the `peopleanalyticsdata` package or downloaded from the internet^[http://peopleanalytics-regression-book.org/data/job_retention.csv].  The `intention` field represents a score of 1 to 10 on the individual's intention to leave their job in the next 12 months, where 1 indicates an extremely low intention and 10 indicates an extremely high intention.  This response was recorded at the beginning of the study period.

1.  Create three categories of `intention` as follows:  High (score of 7 or higher), Moderate (score of 4--6), Low (score of 3 or less)
2.  Calculate Kaplan-Meier survival estimates for the three categories and visualize these using survival curves.
3.  Determine the effect of `intention` on retention using a Cox proportional hazard model, controlling for `gender`, `field` and `level`.
4.  Perform an appropriate check that the proportional hazard assumption holds for your model.
5.  Run a similar model but this time include the `sentiment` input variable.  How would you interpret the results?
6.  Experiment with running a frailty model to take into account the different background attrition risk by field of employment.

