# Alternative Technical Approaches in R and Python {#alt-approaches}

As outlined earlier in this book, all technical implementations of the modeling techniques in previous chapters have relied wherever possible on base R code and specialist packages for specific methodologies---this allowed a focus on the basics of understanding, running and interpreting these models which is the key aim of this book.    For those interested in a wider range of technical options for running inferential statistical models, this chapter illustrates some alternative options and should be considered a starting point for those interested rather than an in-depth exposition.  

First we look at options for generating models in more predictable formats in R.  We have seen in prior chapters that the output of many models in R can be inconsistent.  In many cases we are given more information than we need and in some cases we have less than we need.  Formats can vary and we sometimes need to look in different parts of the output to see specific statistics that we seek.  `r if (knitr::is_latex_output()) '\\index{R!packages!tidymodels@\\texttt{tidymodels}}'`The `tidymodels` set of packages tries to bring the principles of tidy data into the realm of statistical modeling and we will illustrate this briefly.

Second, for those whose preference is to use Python, we provide some examples for how inferential regression models can be run in Python.  While Python is particularly well-tooled for running predictive models, it does not have the full range of statistical inference tools that are available in R.  In particular, using predictive modeling or machine learning packages like `r if (knitr::is_latex_output()) '\\index{Python!packages!scikit-learn@\\texttt{scikit-learn}}'``scikit-learn` to conduct regression modeling can often leave the analyst lacking when seeking information about certain model statistics when those statistics are not typically sought after in a predictive modeling workflow.  We briefly illustrate some Python packages which perform modeling with a greater emphasis on inference versus prediction.  

## 'Tidier' modeling approaches in R

The `tidymodels` meta-package is a collection of packages which collectively apply the principles of tidy data to the construction of statistical models.  More information and learning resources on `tidymodels` can be found `r if (knitr::is_latex_output()) {"at \\texttt{https://www.tidymodels.org/}"} else {"[here](https://www.tidymodels.org/)"}`.  Within `tidymodels` there are two packages which are particular useful in controlling the output of models in R:  the `broom` and `parsnip` packages.

### The `broom` package

`r if (knitr::is_latex_output()) '\\index{R!packages!broom@\\texttt{broom}}'`Consistent with how it is named, `broom` aims to tidy up the output of the models into a predictable format.  It works with over 100 different types of models in R.  `r if (knitr::is_latex_output()) '\\index{data sets!salespeople@\\texttt{salespeople}}'`In order to illustrate its use, let's run a model from a previous chapter---specifically our salesperson promotion model in Chapter \@ref(bin-log-reg).

```{r}
# obtain salespeople data
url <- "http://peopleanalytics-regression-book.org/data/salespeople.csv"
salespeople <- read.csv(url)
```

As in Chapter \@ref(bin-log-reg), we convert the `promoted` column to a factor and run a binomial logistic regression model on the `promoted` outcome.

```{r}
# convert promoted to factor
salespeople$promoted <- as.factor(salespeople$promoted)

# build model to predict promotion based on sales and customer_rate
promotion_model <- glm(formula = promoted ~ sales + customer_rate,
                       family = "binomial",
                       data = salespeople)

```

We now have our model sitting in memory.  We can use three key functions in the `broom` package to view a variety of model statistics.  First, the `tidy()` function allows us to see the coefficient statistics of the model.

```{r}
# load tidymodels metapackage
library(tidymodels)

# view coefficient statistics
broom::tidy(promotion_model)

```

The `glance()` function allows us to see a row of overall model statistics:

```{r}
# view model statistics
broom::glance(promotion_model)
```

The `augment()` function augments the observations in the data set with a range of observation-level model statistics such as residuals:

```{r, eval = FALSE}
# view augmented data
head(broom::augment(promotion_model))
```

`r if (knitr::is_latex_output()) '\\small'`
```{r, echo = FALSE}
head(broom::augment(promotion_model))
```
`r if (knitr::is_latex_output()) '\\normalsize'`

`r if (knitr::is_latex_output()) '\\index{data sets!soccer@\\texttt{soccer}}'`These functions are model-agnostic for a very wide range of common models in R.  For example, we can use them on our proportional odds model on soccer discipline from Chapter \@ref(ord-reg) and they will generate the relevant statistics in tidy tables.

```{r}
# get soccer data
url <- "http://peopleanalytics-regression-book.org/data/soccer.csv"
soccer <- read.csv(url)

# convert discipline to ordered factor
soccer$discipline <- ordered(soccer$discipline, 
                             levels = c("None", "Yellow", "Red"))

# run proportional odds model
library(MASS)
soccer_model <- polr(
  formula = discipline ~ n_yellow_25 + n_red_25 + position + 
    country + level + result, 
  data = soccer
)

# view model statistics
broom::glance(soccer_model)
```


`r if (knitr::is_latex_output()) '\\index{R!packages!dplyr@\\texttt{dplyr}}'``broom` functions integrate well into other tidyverse methods, and allow easy running of models over nested subsets of data.  For example, if we want to run our soccer discipline model across the different countries in the data set and see all the model statistics in a neat table, we can use typical tidyverse grammar to do so using `dplyr`.

`r if (knitr::is_latex_output()) '\\newpage'`

```{r, warning = FALSE, message = FALSE}
# load the tidyverse metapackage (includes dplyr)
library(tidyverse)

# define function to run soccer model and glance at results
soccer_model_glance <- function(form, df) {
  model <- polr(formula = form, data = df)
  broom::glance(model)
}

# run it nested by country
soccer %>% 
  dplyr::nest_by(country) %>% 
  dplyr::summarise(
    soccer_model_glance("discipline ~ n_yellow_25 + n_red_25", data)
  )
```

In a similar way, by putting model formulas in a dataframe column, numerous models can be run in a single command and results viewed in a tidy dataframe.

```{r, eval = FALSE}
# create model formula column
formula <- c(
  "discipline ~ n_yellow_25", 
  "discipline ~ n_yellow_25 + n_red_25",
  "discipline ~ n_yellow_25 + n_red_25 + position"
)

# create dataframe
models <- data.frame(formula)

# run models and glance at results
models %>% 
  dplyr::group_by(formula) %>% 
  dplyr::summarise(soccer_model_glance(formula, soccer))
```
`r if (knitr::is_latex_output()) '\\newpage \\scriptsize'`
```{r, echo = FALSE, warning = FALSE, message = FALSE}
# create model formula column
formula <- c(
  "discipline ~ n_yellow_25", 
  "discipline ~ n_yellow_25 + n_red_25",
  "discipline ~ n_yellow_25 + n_red_25 + position"
)

# create dataframe
models <- data.frame(formula)

# run models and glance at results
models %>% 
  dplyr::group_by(formula) %>% 
  dplyr::summarise(soccer_model_glance(formula, soccer))
```
`r if (knitr::is_latex_output()) '\\normalsize'`

### The `parsnip` package

`r if (knitr::is_latex_output()) '\\index{R!packages!parsnip@\\texttt{parsnip}}'`The `parsnip` package aims create a unified interface to running models, to avoid users needing to understand different model terminology and other minutiae.   It also takes a more hierarchical approach to defining models that is similar in nature to the object-oriented approaches that Python users would be more familiar with.

Again let's use our salesperson promotion model example to illustrate.  We start by defining a model family that we wish to use, in this case logistic regression, and define a specific engine and mode.

```{r}
model <- parsnip::logistic_reg() %>% 
  parsnip::set_engine("glm") %>% 
  parsnip::set_mode("classification")
```

We can use the `translate()` function to see what kind of model we have created:

```{r, eval = FALSE}
model %>% 
  parsnip::translate()
```

`r if (knitr::is_latex_output()) '\\small'`
```{r, echo = FALSE}
model %>% 
  parsnip::translate()
```
`r if (knitr::is_latex_output()) '\\normalsize'`

Now with our model defined, we can fit it using a formula and data and then use `broom` to view the coefficients:

```{r}
model %>% 
  parsnip::fit(formula = promoted ~ sales + customer_rate,
               data = salespeople) %>% 
  broom::tidy()
```

`parsnip` functions are particularly motivated around tooling for machine learning model workflows in a similar way to `scikit-learn` in Python, but they can offer an attractive approach to coding inferential models, particularly where common families of models are used. 

## Inferential statistical modeling in Python

`r if (knitr::is_latex_output()) '\\index{Python!packages!scikit-learn@\\texttt{scikit-learn}}'`In general, the modeling functions contained in `scikit-learn`---which tends to be the go-to modeling package for most Python users---are oriented towards predictive modeling and can be challenging to navigate for those who are primarily interested in inferential modeling.  In this section we will briefly review approaches for running some of the models contained in this book in Python.  `r if (knitr::is_latex_output()) '\\index{Python!packages!statsmodels@\\texttt{statsmodels}}'`The `statsmodels` package is highly recommended as it offers a wide range of models which report similar statistics to those reviewed in this book.  Full `statsmodels` documentation can be found `r if (knitr::is_latex_output()) {"at \\texttt{https://www.statsmodels.org/stable/index.html}"} else {"[here](https://www.statsmodels.org/stable/index.html)"}`.

### Ordinary least squares (OLS) linear regression

The OLS linear regression model reviewed in Chapter \@ref(linear-reg-ols) can be generated using the `statsmodels` package, which can report a reasonably thorough set of model statistics.  By using the `statsmodels` formula API, model formulas similar to those used in R can be used.  

```{python, eval = FALSE}
import pandas as pd
import statsmodels.formula.api as smf

# get data
url = "http://peopleanalytics-regression-book.org/data/ugtests.csv"
ugtests = pd.read_csv(url)

# define model
model = smf.ols(formula = "Final ~ Yr3 + Yr2 + Yr1", data = ugtests)

# fit model
ugtests_model = model.fit()

# see results summary
print(ugtests_model.summary())
```

`r if (knitr::is_latex_output()) '\\small'`
```{python, echo = FALSE}
import pandas as pd
import statsmodels.formula.api as smf

# get data
url = "http://peopleanalytics-regression-book.org/data/ugtests.csv"
ugtests = pd.read_csv(url)

# define model
model = smf.ols(formula = "Final ~ Yr3 + Yr2 + Yr1", data = ugtests)

# fit model
ugtests_model = model.fit()

# see results summary
print(ugtests_model.summary())
```
`r if (knitr::is_latex_output()) '\\normalsize'`


### Binomial logistic regression

Binomial logistic regression models can be generated in a similar way to OLS linear regression models using the `statsmodels` formula API, calling the binomial family from the general `statsmodels` API.

```{python, eval = FALSE}
import pandas as pd
import statsmodels.api as sm
import statsmodels.formula.api as smf

# obtain salespeople data
url = "http://peopleanalytics-regression-book.org/data/salespeople.csv"
salespeople = pd.read_csv(url)

# define model
model = smf.glm(formula = "promoted ~ sales + customer_rate", 
                data = salespeople, 
                family = sm.families.Binomial())


# fit model
promotion_model = model.fit()


# see results summary
print(promotion_model.summary())
```

`r if (knitr::is_latex_output()) '\\small'`
```{python, echo = FALSE}
import pandas as pd
import statsmodels.api as sm
import statsmodels.formula.api as smf

# obtain salespeople data
url = "http://peopleanalytics-regression-book.org/data/salespeople.csv"
salespeople = pd.read_csv(url)

# define model
model = smf.glm(formula = "promoted ~ sales + customer_rate", 
                data = salespeople, family = sm.families.Binomial())

# fit model
promotion_model = model.fit()

# see results summary
print(promotion_model.summary())
```
`r if (knitr::is_latex_output()) '\\normalsize'`

### Multinomial logistic regression

Multinomial logistic regression is similarly available using the `statsmodels` formula API.  As usual, care must be taken to ensure that the reference category is appropriately defined, dummy input variables need to be explicitly constructed, and a constant term must be added to ensure an intercept is calculated.

```{python, eval = FALSE}
import pandas as pd
import statsmodels.api as sm

# load health insurance data
url = "http://peopleanalytics-regression-book.org/data/health_insurance.csv"
health_insurance = pd.read_csv(url)

# convert product to categorical as an outcome variable
y = pd.Categorical(health_insurance['product'])

# create dummies for gender
X1 = pd.get_dummies(health_insurance['gender'], drop_first = True)

# replace back into input variables 
X2 = health_insurance.drop(['product', 'gender'], axis = 1)
X = pd.concat([X1, X2], axis = 1)

# add a constant term to ensure intercept is calculated
Xc = sm.add_constant(X)

# define model
model = sm.MNLogit(y, Xc)

# fit model
insurance_model = model.fit()

# see results summary
print(insurance_model.summary())

```

`r if (knitr::is_latex_output()) '\\newpage \\small'`
```{python, echo = FALSE, results = FALSE}
import pandas as pd
import statsmodels.api as sm

# load health insurance data
url = "http://peopleanalytics-regression-book.org/data/health_insurance.csv"
health_insurance = pd.read_csv(url)

# convert product to categorical as an outcome variable
y = pd.Categorical(health_insurance['product'])

# create dummies for gender
X1 = pd.get_dummies(health_insurance['gender'], drop_first = True)

# replace back into input variables 
X2 = health_insurance.drop(['product', 'gender'], axis = 1)
X = pd.concat([X1, X2], axis = 1)

Xc = sm.add_constant(X)

# define model
model = sm.MNLogit(y, Xc)

# fit model
insurance_model = model.fit()
```

```{python, echo = FALSE}
# see results summary
print(insurance_model.summary())
```
`r if (knitr::is_latex_output()) '\\normalsize'`

### Structural equation models

`r if (knitr::is_latex_output()) '\\index{Python!packages!semopy@\\texttt{semopy}}'`The `semopy` package is a specialized package for the implementation of Structural Equation Models in Python, and its implementation is very similar to the `lavaan` package in R.  However, its reporting is not as intuitive compared to `lavaan`.  A full tutorial is available `r if (knitr::is_latex_output()) {"at \\texttt{https://semopy.com/tutorial.html}"} else {"[here](https://semopy.com/tutorial.html)"}`.  Here is an example of how to run the same model as that studied in Section \@ref(struc-eq-model) using `semopy`.

`r if (knitr::is_latex_output()) '\\newpage \\small'`
```{python, results = "hide"}
import pandas as pd
from semopy import Model


# get data
url = "http://peopleanalytics-regression-book.org/data/politics_survey.csv"
politics_survey = pd.read_csv(url)


# define full measurement and structural model
measurement_model = """
# measurement model
Pol =~ Pol1 + Pol2
Hab =~ Hab1 + Hab2 + Hab3
Loc =~ Loc2 + Loc3
Env =~ Env1 + Env2
Int =~ Int1 + Int2
Pers =~ Pers2 + Pers3
Nat =~ Nat1 + Nat2
Eco =~ Eco1 + Eco2

# structural model
Overall ~ Pol + Hab + Loc + Env + Int + Pers + Nat + Eco
"""

full_model = Model(measurement_model)


# fit model to data and inspect
full_model.fit(politics_survey)
```
`r if (knitr::is_latex_output()) '\\normalsize'`

Then to inspect the results:

```{python}
# inspect the results of SEM (first few rows)
full_model.inspect().head()
```

### Survival analysis

`r if (knitr::is_latex_output()) '\\index{Python!packages!semopy@\\texttt{lifelines}}'`The `lifelines` package in Python is designed to support survival analysis, with functions to calculate survival estimates, plot survival curves, perform Cox proportional hazard regression and check proportional hazard assumptions.  A full tutorial is available `r if (knitr::is_latex_output()) {"at \\texttt{https://lifelines.readthedocs.io/en/latest/index.html}"} else {"[here](https://lifelines.readthedocs.io/en/latest/index.html)"}`.  

Here is an example of how to plot Kaplan-Meier survival curves in Python using our Chapter \@ref(survival) walkthrough example. The survival curves are displayed in Figure \@ref(fig:survival-python).

`r if (knitr::is_latex_output()) '\\small'`
```{python, eval = FALSE}
import pandas as pd
from lifelines import KaplanMeierFitter
from matplotlib import pyplot as plt

# get data
url = "http://peopleanalytics-regression-book.org/data/job_retention.csv"
job_retention = pd.read_csv(url)

# fit our data to Kaplan-Meier estimates
T = job_retention["month"]
E = job_retention["left"]
kmf = KaplanMeierFitter()
kmf.fit(T, event_observed = E)

# split into high and not high sentiment
highsent = (job_retention["sentiment"] >= 7)


# set up plot
survplot = plt.subplot()

# plot high sentiment survival function
kmf.fit(T[highsent], event_observed = E[highsent], 
label = "High Sentiment")

kmf.plot_survival_function(ax = survplot)

# plot not high sentiment survival function
kmf.fit(T[~highsent], event_observed = E[~highsent], 
label = "Not High Sentiment")

kmf.plot_survival_function(ax = survplot)

# show survival curves by sentiment category
plt.show()
```
`r if (knitr::is_latex_output()) '\\normalsize'`

```{python survival-python, fig.align = "center", fig.cap = "Survival curves by sentiment category in the job retention data", out.width = if (knitr::is_latex_output()) {"90%"}, echo = FALSE, results = FALSE, dev = "png", dev.args = list(type = "cairo-png")}
import pandas as pd
import matplotlib
from lifelines import KaplanMeierFitter
from matplotlib import pyplot as plt
matplotlib.rcParams['pdf.fonttype'] = 42
matplotlib.rcParams['ps.fonttype'] = 42
matplotlib.rcParams['text.usetex'] = True

# get data
url = "http://peopleanalytics-regression-book.org/data/job_retention.csv"
job_retention = pd.read_csv(url)

# fit our data to Kaplan-Meier estimates
T = job_retention["month"]
E = job_retention["left"]
kmf = KaplanMeierFitter()
kmf.fit(T, event_observed = E)

# split into high and not high sentiment
highsent = (job_retention["sentiment"] >= 7)

# set up plot
survplot = plt.subplot()

# plot high sentiment survival function
kmf.fit(T[highsent], event_observed = E[highsent], 
label = "High Sentiment")

kmf.plot_survival_function(ax = survplot)

# plot not high sentiment survival function
kmf.fit(T[~highsent], event_observed = E[~highsent], 
label = "Not High Sentiment")

kmf.plot_survival_function(ax = survplot)

# show survival curves by sentiment category
plt.show()
```



And here is an example of how to fit a Cox Proportional Hazard model similarly to Section \@ref(coxphmodel)^[I am not aware of any way of running frailty models currently in Python].

```{python, results = "hide"}
from lifelines import CoxPHFitter

# fit Cox PH model to job_retention data
cph = CoxPHFitter()
cph.fit(job_retention, duration_col = 'month', event_col = 'left', 
        formula = "gender + field + level + sentiment")
```

```{python, eval = FALSE}
# view results
cph.print_summary()
```

`r if (knitr::is_latex_output()) '\\newpage \\tiny'`
```{python, echo = FALSE}
# view results
cph.print_summary()
```
`r if (knitr::is_latex_output()) '\\normalsize'`

Proportional Hazard assumptions can be checked using the `check_assumptions()` method^[Schoenfeld residual plots can be seen by setting `show_plots = True` in the parameters].

```{python, eval = FALSE}
cph.check_assumptions(job_retention, p_value_threshold = 0.05)
```

`r if (knitr::is_latex_output()) '\\newpage \\scriptsize'`
```{python, echo = FALSE}
cph.check_assumptions(job_retention, p_value_threshold = 0.05)
```
`r if (knitr::is_latex_output()) '\\normalsize'`

### Other model variants

Implementation of other model variants featured in earlier chapters becomes thinner in Python.  However, of note are the following:

* Ordinal regression is not currently available in the release version of the `statsmodels` package, but is available in the development version.  The `mord` package offers an implementation of ordinal regression for predictive analytics purposes, but for inferential modeling users will need to wait for a release of `statsmodels` that contains ordinal regression methods or for immediate use they will need to install the development version from source.

* Mixed models only currently have an implementation for linear mixed modeling in `statsmodels`.  Generalized linear mixed models equivalent to those found in the `lme4` R package are not yet available in Python.




